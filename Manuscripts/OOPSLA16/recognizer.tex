Formal presentation of the algorithm that compiles an LL(1) grammar into an implementation of a
  language recognizer with \Java generics
  is delayed to the next section.

This section gives an intuitive perspective on this algorithm:
We will first recall the essentials of the classical LL(1) parsing algorithm.
Then, we will explain the limitations of the computational model
  offered by \Java generics.
The discussion proceeds to the observation
  that underlies our emulation of the parsing algorithm
  within these limitations.
Next, we revise the classical algorithm for converting
  an LL(1) grammar into an LL(1) parser, and explain
  how it is modified to generate a recognizer.

An LL(1) parser is a DPDA allowed to peek at the next input terminal.
Thus,~$δ$, its transition function, takes two parameters: the current state of
of the automaton, and a peek into the terminal next to be read.
The actual operation of the automaton
  is by executing in loop the step
    depicted in \cref{algorithm:ll-parser}.

\renewcommand\algorithmicdo{\textbf{\emph{}}}
\renewcommand\algorithmicthen{\textbf{\emph{}}}
\begin{algorithm}[p]
  \caption{\label{algorithm:ll-parser}
  DPDA algorithm for LL(1) parsing}
  \begin{algorithmic}
      \LET{$X$}{\Function pop()} \COMMENT{What's anticipated on input?}
      \IF[ancipated~$X$ is a terminal]{$X∈Σ$}
      \IF[read terminal is not anticipated~$X$]{$X≠\Function next()$}
      \STATE{\textsc{Reject}} \COMMENT{automaton halts in error}
      \ELSE[terminal just read was the anticipated~$X$]
      \STATE{\textsc{Continue}} \COMMENT{restart, popping a new~$X$, etc.}
        \FI
      \ELSIF[anticipating end-of-file]{$X=\$$}
        \IF[not the anticipated end-of-file]{$\$≠\Function next()$}
          \STATE{\textsc{Reject}}\COMMENT{automaton halts in error}
          \ELSE[terminal just read was the anticipated~$X$]
          \STATE{\textsc{Accept}}\COMMENT{All input successfully consumed}
        \FI
      \ELSE[anticipated terminal~$X$ must be a nonterminal]
      \LET{$R$}{$\Function δ(X, \Function peek())$} \COMMENT{Which rule to reduce?}
      \IF[No rule found]{$R=⊥$}
          \STATE{\textsc{Reject}}\COMMENT{automaton halts in error}
        \ELSE[A rule was found]
        \LET{$[Z→Y₁,…,Yₖ]$}{R} \COMMENT{break~$R$ into left/right}
      \STATE{\textbf{assert}~$Z=X$} \COMMENT{$δ$ constructed to return valid~$R$ only}
          \STATE{$\Function print(R)$} \COMMENT{rule~$R$ has just been applied}
          \STATE{\textbf{For}~$i=k,…,1$,~$\Function push(Yᵢ)$}
          \COMMENT[push in reverse order]
        \FI
      \FI
\end{algorithmic}
  \vspace{0.3ex}
  \hrule
  \vspace{0.3ex}
  \scriptsize
  \begin{enumerate}
      \item
  Input is a stream of terminals drawn from alphabet~$Σ$, ending
  with a special symbol~$\$\ni Σ$.
      \item
  Stack symbols are drawn from~$Σ∪❴\$❵∪Ξ$, where~$Ξ$ is
the set of nonterminals of the grammar from which the automaton was generated.
\item
  Functions~$\Function pop()$ and~$\Function push(·)$
  operate on the push-down stack; function~$\Function next()$ returns
  and consumes the next terminal from the input stream;
  function~$\Function peek()$ returns this terminal without consuming it.
\item
The automaton starts with the stack with the start symbol~$S∈Ξ$ pushed
  into its stack.
  \end{enumerate}
\end{algorithm}

As seen in the figure, the DPDA pops its state from the
  stack.
If the state is an input terminal or the special, end-of-file,
  symbol~$\$$, then it must match
  the next terminal found in the input stream.
If no match is found, then the parser rejects.
The parser accepts if the input is exhausted with
  no rejections.

The more interesting case is that~$X$, the popped symbol
  is a nonterminal.
If this is the case, then the DPDA peeks into the next terminal in the input
  stream (without consuming it).
Based on this terminal, and~$X$ the transition function~$δ$
  determines~$R$ the derivation rule to use in order to derive~$X$.
The algorithm rejects if~$δ$ can offer no such rule.
Otherwise, it pushes into the stack, in reverse order, the symbols
  found in the right hand side of~$R$.

Can \cref{Algorithm:ll-parser} be executed on the machinery
  available with \Java generics?
As it turns out, most operations conducted by the algorithm
  are easy.
The implementation of function~$δ$ can
  be found in the toolbox.
Similarly, any fixed sequence of push and pop
  operations on the stack can be conducted within a \Java
  transition function:
  the ‟\textbf{For}" at the algorithm can be unrolled.

The algorithm may appears to be doing a constant amount
  of work for each input symbol.
However, a little scrutiny falsifies any such conclusion.

First, if we look at the case~$R=X→Y$,~$Y$
  being a nonterminal then the algorithm will conduct two~$\Function push()$ operations,
  one for~$X$ and one for~$Y$ before consuming a terminal from the input.
In fact, the number of such push operations is hard to bind since function~$δ$ may
  return the rule~$Y→Z$, and then~$Z→Z'$ etc.

Second issue is the interesting case that~$k=0$.
Should this happen, the DPDA does not push anything into the stack
  and proceeds, in the next iteration, to consider the next symbol
  popped from the stack.
Again, the number of pop operations is hard to bind since the next element
  popped from the stack could be yet another nonterminal for each function~$δ$
  returns again a rule whose right-hand side is empty.

These two issues occur frequently in grammars.
\Cref{Figure:issue-1} shows an example based on the original
  grammar of \Pascal \urlref{original-full-specification}.
Some of the~$δ$ function values for this grammar are
\begin{equation}
δ = 3 !
\end{equation}

\newsavebox{\Alphabet}
\begin{lrbox}{\Alphabet}
  \begin{tabularx}{0.40\linewidth}{l}
    \cc{program}, \cc{begin}, \cc{end},⏎
    \cc{label}, \cc{const}, \cc{id},⏎
    \cc{procedure},~\cc{;},~\cc{(}, \cc{()}
  \end{tabularx}
        \end{lrbox}

\begin{figure}
  \caption{\label{Figure:running}
    An LL(1) grammar over the alphabet
    \[
      Σ = \left❴\usebox\Alphabet\right❵.
    \]
    (inspired by the original \Pascal grammar; to serve as
    our running example)
  }
  \begin{Grammar}
    \begin{aligned}
      \<Program> & ::= \cc{program} \cc{id} \<Parameters>~\cc{;} \<Definitions>\~\<Body>\hfill⏎
      \<Body> & ::= \cc{begin} \cc{end}\hfill⏎
      \<Definitions> & ::= \<Labels>\~\<Constants>\~\<Nested>\hfill⏎
      \<Labels> & ::= ε \| \cc{label} \<Label>\~\<MoreLabels> \hfill⏎
      \<Constants> & ::= ε \| \cc{const} \<Constant>\~\<MoreConstants> \hfill⏎
      \<Label> & ::=\cc{;} \hfill⏎
      \<Constant> & ::=\cc{;} \hfill⏎
      \<MoreLabels> & ::= ε \| \<Label>\~\<MoreLabels>\hfill⏎
      \<MoreConstants> & ::= ε \| \<Constant>\~\<MoreConstants>\hfill⏎
      \<Nested> & ::= ε \| \<Procedure>\~\<Nested> \hfill⏎
      \<Procedure> & ::= \cc{procedure} \cc{id} \<Parameters>~\cc{;} \<Definitions>~\cc{(} \<Body> \hfill⏎
      \<Parameters> & ::= ε \| \cc{()} \hfill⏎
    \end{aligned}
  \end{Grammar}
\end{figure}
If this is the case, \cref{algorithm:ll-parser}
pops an element out of the stack, without pushing any more elements into it
Note that the fact
The implementation of the LL parser as recognizer without using
a jump-stack is problematic since LL grammars allow~$ε$-rules.
During the processing of the~$ε$-rule, the parser pops the
left-hand side of the rule and continues to process the penultimate
stack element.
It is possible to have multiple~$ε$ rules processed sequentially,
that leads to the \textit{realtime} problem - we can only encode
automata that do constant amount of computation on each input character
process.

\begin{figure}[H]
  \begin{Grammar}
    \begin{aligned}
      \<BNF> & ::= \<Header>\~\<Body>\~\<Footer> \hfill⏎
      \<Header> & ::= \<Variables> \~\<Terminals> \hfill⏎
      {} & \| \<Terminals> \~\<Variables> \hfill⏎
      \<Variables> & ::= \cc{with(Class<? \kk{extends} Variable>)}\hfill⏎
      \<Terminals> & ::= \cc{with(Class<? \kk{extends} Terminal>)}\hfill⏎
      \<Body> & ::= \<Start> \~\<Rules> \hfill⏎
      \<Start> & ::= \cc{start(\<Variable>)} \hfill⏎
      \<Rules> & ::= \<Rule> \~\<Rules>\hfill⏎
      {} & \| \<Rule> \hfill⏎
      \<Rule> & ::= \cc{derives(\<Variable>)} \<Conjunctions>\hfill⏎
      \<Conjunctions> & ::= \<First-Conjunction>\~\<Extra-Conjunctions>\hfill⏎
      \<First-Conjunction> & ::= \cc{to(\<Symbol>)}\~\<Symbol-Sequence>\hfill⏎
      {} & \| \cc{toNone()}\hfill⏎
      \<Extra-Conjunctions> & ::= \<Extra-Conjunction>\~\<Extra-Conjunctions>\hfill⏎
      {} & \| ε\hfill⏎
      \<Extra-Conjunction> & ::= \cc{or(\<Symbol>)}\~\<Symbol-Sequence>\hfill⏎
      {} & \| \cc{orNone()} \hfill⏎
      \<Symbol-Sequence> & ::= ε \hfill⏎
      {} & \| \cc{and(\<Symbol>)}\~\<Symbol-Sequence> \hfill⏎
      \<Symbol> & ::= \cc{Variable} \hfill⏎
      {} & \| \<Verb>\hfill⏎
      {} & \| \<Verb>~\cc{,} \<Noun> \hfill⏎
      \<Noun> & ::= \<Variable> \hfill⏎
      {} & \| \<Existing-Class> \hfill⏎
      \<Variable> & ::= \cc{Variable} \hfill⏎
      \<Footer> & ::= \cc{go()}\hfill⏎
    \end{aligned}
  \end{Grammar}
  \caption{A BNF grammar for defining BNF grammars}
  \label{Figure:BNF:BNF}
\end{figure}
%How does the LL parser work?
During normal LL parsing, the configuration of the parser consists
  of three elements:
\begin{itemize}
  \item the current state of the parsing - which is the
    rule being currently processed and the part of the rule we
    haven't parsed yet.
  \item the environment, or the parsing information we got so far -
    which is the rest of the stack
  \item the rest of the input string
\end{itemize}

% How can we encode it?
The corresponding \Java fluent API ought to have equivalent features.
The input string - since we transform each
  terminal to a method invocation, it is clear that the rest of the input is
  simply the rest of the method chain.
The current state of the parsing is transformed into a \Java type
  (\kk{class} or \kk{interface}) ; this way we can control for each state
  which operation can be made by determining the method of that type.
The environment is realized by type arguments of the states' \Java types.

Using these transformers, we can describe a state of the parser,
  with different stacks using a single type.
This is of course necessary in order to describe all posible configurations
  in a finite number of \Java types.

%
\subsection{LL states}
A state during the parser's computation is a grammar rule~$r = A→αβ$, and an
index in it, i.e.,~$A→α·β$.
It means that the parser is currently processing rule~$r$, it already realized parsed string~$alpha$
  from the input, and it expect to parse the string~$β$ next.
The number of states then, produced by \Fajita for a grammar is linear in the grammar size.

\subsection{Environment}

\begin{algorithm}[p]
  \caption{\label{algorithm:llclosure}
  function~$\Function closure(a,b)$: generates a closure of action from the original ll algorithm}
  \begin{algorithmic}
    \INPUT{a nonterminal~$a$}
    \INPUT{a terminal~$b$}
    \OUTPUT{the closure of consecutive actions}
    \LET{$L$}{$[]$}
    \LET{$x$}{$a$}
    \IF{$∀x→α . b∉\Function first(α)$}
      \RETURN{\textrm{reject}}
    \FI
    \WHILE{$\textrm{true}$}
      \IF{$x∈ξ$}
        \STATE{let~$y₁ y₂… yₖ$ be~$α$ s.t.~$∃x→α∈g∧b∈\Function first(α)$}
        \STATE{$\Function append(l,yₖ,y_{k-1},…,y₂)$}
        \STATE{$x=y₁$}
      \ELSE[$x$ must be~$b$]
        \BREAK
      \FI
    \DONE
    \RETURN{$L$}
  \end{algorithmic}
\end{algorithm}

\endinput

\begin{Definition}[JETEM]
  JETEM is a variant of pushdown automata that maintains two
    stacks:
    \begin{description}
      \item[Call stack]
      \item[Epsilon stack]
    \end{description}
    At each step of the automaton, it reads a single character,
      and determines based on the character and the current top of the call stack, what to do:
    \begin{description}
      \item[Epsilon] if character is not in first, but is in follow
      \item[Call] if character is in first but not in follow
      \item[Reject] if character is not in first nor in follow
      \item[Accept] when input exhausted.
    \end{description}

  \begin{enumerate}
      \item

\end{Definition}

\begin{figure}[H]
  \begin{Grammar}
    \begin{aligned}
      \<BNF> & ::= \<Header>\~\<Body>\~\<Footer> \hfill⏎
      \<Header> & ::= \<Variables> \~\<Terminals> \hfill⏎
      {} & \| \<Terminals> \~\<Variables> \hfill⏎
      \<Variables> & ::= \cc{with(Class<? \kk{extends} Variable>)}\hfill⏎
      \<Terminals> & ::= \cc{with(Class<? \kk{extends} Terminal>)}\hfill⏎
      \<Body> & ::= \<Start> \~\<Rules> \hfill⏎
      \<Start> & ::= \cc{start(\<Variable>)} \hfill⏎
      \<Rules> & ::= \<Rule> \~\<Rules>\hfill⏎
      {} & \| \<Rule> \hfill⏎
      \<Rule> & ::= \cc{derives(\<Variable>)} \<Conjunctions>\hfill⏎
      \<Conjunctions> & ::= \<First-Conjunction>\~\<Extra-Conjunctions>\hfill⏎
      \<First-Conjunction> & ::= \cc{to(\<Symbol>)}\~\<Symbol-Sequence>\hfill⏎
      {} & \| \cc{toNone()}\hfill⏎
      \<Extra-Conjunctions> & ::= \<Extra-Conjunction>\~\<Extra-Conjunctions>\hfill⏎
      {} & \| ε\hfill⏎
      \<Extra-Conjunction> & ::= \cc{or(\<Symbol>)}\~\<Symbol-Sequence>\hfill⏎
      {} & \| \cc{orNone()} \hfill⏎
      \<Symbol-Sequence> & ::= ε \hfill⏎
      {} & \| \cc{and(\<Symbol>)}\~\<Symbol-Sequence> \hfill⏎
      \<Symbol> & ::= \cc{Variable} \hfill⏎
      {} & \| \<Verb>\hfill⏎
      {} & \| \<Verb>~\cc{,} \<Noun> \hfill⏎
      \<Noun> & ::= \<Variable> \hfill⏎
      {} & \| \<Existing-Class> \hfill⏎
      \<Variable> & ::= \cc{Variable} \hfill⏎
      \<Footer> & ::= \cc{go()}\hfill⏎
    \end{aligned}
  \end{Grammar}
  \caption{A BNF grammar for defining BNF grammars}
  \label{Figure:BNF:BNF}
\end{figure}
