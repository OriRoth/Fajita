\Cref{Theorem:Gil-Levy} and its proof above also provide
an algorithm for converting an EBNF specification of a fluent API into its
its realization:
\begin{quote}
  \begin{enumerate}
    \item Convert the specification into a plain BNF form.~\cite{CONVERT}
    \item Convert this BNF into a definition of a DPDA. If conversion fails,
      then the given specification is not deterministic context free. ~\cite{C1}
    \item Convert this DPDA into a jDPDA. (Conversion is guaranteed to succeed.) ~\cite{CONVERT}
    \item Apply the proof to generate appropriate \Java type definitions, making sure to
        augment methods with code to maintain the fluent-call-list.
        Parsing the fluent-call-list can done either a each method,
        or lazily, when the product of the fluent API call chain is to
         be used.
  \end{enumerate}
\end{quote}

The \Java compiler is the main computational tool we use
in this manuscript.
In particular, the \Java generics mechanism is what
boosted our expressiveness from the trivial \emph{regular languages}
set, to the practical, reasonable \emph{deterministic context free languages} set.



An interesting question that was raised during this research,
  is what the runtime complexity of the \Java compiler is.
The reason this question is interesting, is its implication
  on the computational expressiveness of type-encoding.

For example, if we recognized that the \Java parser spends
  linear time on its input, we could say that it's not
  likely that we can type-encode nondeterministic CFG\@.
The reason is the fact that the best known algorithms
  today for parsing general nondeterministic CFG,
  run in super quadratic time.
And it is highly unlikely that the type-checker of \Java incidentally
  found an algorithm that is practically perfect in big-O notation.

As we explored this venue, we discovered that the type-checker of
  \Java actually runs in exponential time.

Consider an encoding of an S-expression in type~\cc{Cons}
  defined in~\cref{Figure:compiler}.

\begin{wrapfigure}[6]r{27ex}
  \caption{\label{Figure:compiler} Encoding of an binary type tree}
  \javaInput[minipage,width=27ex,left=-2ex]{compiler.listing}
\end{wrapfigure}

Type \cc{Cons} takes two type parameters, \cc{Car} and \cc{Cdr} (denoting left and right branches).
Denote the return type of \cc{d()} is~$τ= \cc{Cons< Cons<Car, Cdr>, Cons<Car, Cdr> >}$.
Let~$σ$ denote the type of the \kk{this} implicit parameter to~\cc{d}.
Since~$τ= \cc{Cons<}σ,σ\cc{>}$, we have~$|τ|≥2|σ|$,
  where the size of a type is measured, e.g., in number of characters in its textual representation.
In a chain of~$n$ calls to \cc{d()}
\begin{equation}
  \label{Equation:n}
  \cc{(Cons<?,?>(null)).}\overbrace{\cc{d().}⋯\cc{.d()}}^{\text{$n$ times}}\cc{;}
\end{equation}
the size of the resulting type is~$O(2ⁿ)$.

\begin{wrapfigure}r{43ex}%
  \begin{minipage}{43ex}
  \caption{\label{Figure:compile-empiric} Compilation time
    (sec†{measured on an Intel i5-2520M CPU @ 2.50GHz~$⨉$4, 3.7GB memory, Ubuntu 15.04 64-bit, \texttt{javac} 1.8.0\_66}%
    ) \emph{vs.}
      length of call chain.
}
  \gnuplotloadfile[terminal=pdf,terminaloptions={crop size 2.5in,1.5in color enhanced font ",8" linewidth 1}]{../Figures/kill.gnuplot}
\end{minipage}
\end{wrapfigure}%

\Cref{Figure:compile-empiric} shows on the doubly logarithmic plane that the runtime (on a Lenovo X220)
of the \texttt{javac} compiler (version 1.8.0\_66) in face of a \Java program
  assembled from \cref{Figure:compiler} and \cref{Equation:n} placed as the
  single command of \cc{main()}.
We see that this runtime is asymptotically exponential.
(In fact, a variation of the construction may lead to even super-exponential growth rate of the size of types.)

We believe that this exponential growth is due to a design decision of the compiler.
Had the compiler used a representation of types that allows sharing of,
The footprint of exponentially sized types created by can be made linear
  with appropriate sharing of constituent types.
The above discussion, formalism and conjecture may seem at first sight too abstract
  and of little practical value.
Re-positioning perspective may shed a different light:~\cref{Theorem:Gutterman} is important not only because it tells us
  that the question of termination of a \CC compiler is as difficult
  the halting problem~\cite{Turing:1936}, but also because it
  justifies the high resource investment in
  template programming~\cite{Musser:Stepanov:1989,Dehnert:Stepanov:2000
  ,Backhouse:Jansson:1999, Austern:1998,Bracha:Odersky:Stoutamire:Wadler:98,X:Garcia:Jarvi:Lumsdaine:Siek:Willcock:03}.

In the fashion,~\Cref{Theorem:Gil-Levy} has important
  implications to the emerging trend of employing fluent APIs
  to make complex software systems more accessible.
The theoretical part of this work dwells on the proof of~\cref{Theorem:Gil-Levy}.

Accordingly, the contribution of this work is double folded:
  gaining better understanding of the computational expressiveness of
  \Java generics and type hierarchy, and, a better tool
  for designing, experimenting with and perfecting fluent APIs.

One may ask whether our theoretical result is the
  best possible:
It is possible to implement a fluent API for general
  (that is, nondeterministic) context free languages?
  We give reason why this possibility is unlikely.

\subsection{Further Research}
Note the upcoming \textsf{Fajita}?

The algorithm for generating the pushdown automaton is unfortunately impractical.
It is indeed polynomial time and space,
  yet a consumer of formidable resources.
