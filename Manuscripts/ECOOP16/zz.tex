% Conclusions

Fluent are neat. being easy to use, read, and write (thanks to IDEs)

Most reasonable fluent APIs can be realized theoretically(if its LR($k$) for some~$k$ , result from ECOOP)

The class of ``regular'' APIs can be realized easily (as seen in \cref{section:example})
But, defining the specifications of these APIs is hard (as there is not fluent API for regular expressions)

Realizing general fluent APIs automatically can be (very!) hard (abundance of algorithms...)

We did it! to the class of LL(1) grammars.

% Future work

LL(1) is nice .. what about LR(1), LALR or even LL($k$) for $k>1$ ?

We used the type arguments to keep a compile-time stack,
  it is known that a PDA with two stacks is Turing-complete.
Can a second stack be emulated with more type arguments? (as far as i know, no. you can't coordinate between the stacks)
  if so, what is the expressive power of the \Java system?

Can the recognizer provide some runtime information to build a parse tree (or AST) without running 
  an orthogonal parser at runtime?

How can our algorithm or \Fajita itself be employed on other programming languages like \CS.

% Taken from ECOOP
The main contribution of this work is the proof that
  most useful grammars have a fluent API\@.
This brings good news to library designers laboring
  at making their API slick, accessible, and more
  robust to mistakes of clients:
If your API can be phrased in terms of a ‟decent”
  BNF, do not lose hope; the task may be Herculean, but it is (most likely) possible.

Other practitioners may appreciate the toolbox of type encodings offered here
  gaining better understanding of the computational expressiveness of
  \Java generics and type hierarchy, and, a better tool
  for designing, experimenting with and perfecting fluent APIs.

However, once possibility was demonstrated theoretically, the next \emph{research}
    challenge is in an actual fluent API compiler based on lighter weight
  parsing algorithms.
Precisely, the challenge
  is in developing a parsing (or at least recognition)
  algorithm which is not only efficient but also falls within the limited computing power of the \Java types.

On the theoretical front, one may ask whether
  our result is the best possible:
Can the \Java type system be coerced to recognize
  general (that is, nondeterministic) context-free languages?

As mentioned earlier, to the best of our knowledge,
  the complexity of the \Java type checker has never been analyzed.
In light of the empirical finding in \cref{Section:applicability},
  research in this direction may be worthwhile.

Other directions include
  formalizing the proof in \cref{Section:proof} here and
  extensions to other languages.

On a philosophical perspective, several modern programming languages
  acquire high-level constructs at a staggering rate
  (\CC and \Scala being prominent examples).
The main yardstick for evaluation these
  is ‟programmer's convenience”.
This work suggests an orthogonal perspective, namely
  computational expressiveness, or, stated differently,
    ranking of a new construct by its ability to recognize languages
    in the Chomsky hierarchy~\cite{Chomsky:1963}.
