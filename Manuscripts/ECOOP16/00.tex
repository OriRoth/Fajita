\title{%
  \Self \protect \thanks {%
    \textbf
    Fluent \textbf API for \textsc{\textbf Java}
    (\textbf Inspired by the \textbf Theory of \textbf Automata)
  }
  \newline
  \color{red}{%
    \rmfamily\scshape Thou Mortal, Be Warned. \newline
    Thou Shallt Not Remove \newline
    This Commandment \newline
    While There Are Signs of Haste \newline
    in This Document!!!!\newline
  }
}

\documentclass[a4paper,USenglish]{lipics}
\usepackage{\jobname}

%\author{Tome Levy⏎
% Department of Computer Science⏎
% Technion---Israel Institute of Technology⏎
% \texttt{\small \href{mailto:stlevy@campus.technion.ac.il}{stlevy@campus.technion.ac.il}}}
%\date{%
% Research Proposal⏎
% \small Advised by Prof.\ Yossi Gil
%}

\author{Anonymized for the submission}

\begin{document}
\maketitle
\begin{abstract}
  \input{abstract}
\end{abstract}

\section{Introduction}
\input{aa}

\textbf{Outline.}
The first main result is presented in~\cref{Section:proof}.
Towards this,~\cref{Section:preliminaries}, the preliminaries, 
  recalls the central notions we rely on: DSL, fluent API,
  context free languages, pushdown automata, etc. 
At the end of this section, the accumulated vocabulary is used to state this
  result more formally.
This terminology is used again in \Cref{Section:related} to offer 
  a perspective on related work.
\Cref{Section:toolkit} then builds a small toolkit of idioms and techniques
  for programming generics in \Java.
This toolkit is used in the subsequent~\cref{Section:proof} for
  proving our main theoretical result.
\Cref{Section:bridge} explains then why
  this theoretical result is unlikely to be improved.
This section also explains why the expressive power of \Self
  is unlikely to be improved much.

\Cref{Section:fajita} introduces \Self.
The algorithm that drives \Self, of generating the complex
  type encoding behind the parser generator
  is described in~\cref{Section:algorithm}.
The main ideas behind the bootstrapping definition of \Self
  are revealed in~\Cref{Section:bootstrapping}.
\Cref{Section:AST} shows how \Self can be used not only
  for recognizing the input language,
  but also for creating the appropriate parse tree, and even the \emph{\textbf Abstract \textbf Syntax \textbf Tree} (AST):
  The fluent API code generated by \Self builds,
    when run, the AST of any permissible call chain.
    And, of course, this AST is compliant with the BNF definition
      of the fluent API.

\section{Reminders and Preliminaries}
\label{Section:preliminaries}
\input{preliminaries}

\section{Related Work}
\label{Section:related}
\input{related}

\section{Toolkit for Type Encoding}
\label{Section:toolkit}
\input{toolkit}


\section{The Jump-Stack Data-Structure}
\label{Section:jump}
\input{jump}

\section{Proof of \Cref{Theorem:Gil-Levy}}
\label{Section:proof}
\input{proof}

\section{Parser of Non Deterministic Pushdown Automata}
\label{Section:bridge}
\input{bridge}

\section{Introducing \Self}
\label{Section:fajita}
\input{fajita}

\section{Algorithm}
\label{Section:algorithm}
\input{algorithm}

\section{Bootstrapping Definition}
\label{Section:bootstrapping}
\input{bootstrapping}

\section{Using \Self for Producing ASTs}
\label{Section:AST}
\input{AST}

\section{Conclusion}
\label{Section:zz}
%\input{zz}

\bibliographystyle{abbrv}\small
\bibliography{author-names,other-shorthands,publishers-abbreviated,journals-abbreviated,yogi-book,00}
\end{document}

Processing programming languages
\begin{description}
  \item[Lexical analysis] - the first step of the process in which the character strings generated by the
  programmer are aggregated to the abstract tokens defined by the language designer.
  \item[Syntactical analysis (parsing) ] - the second step, in which the processed strings of tokens
  conform to the rules of a formal grammar defined by the language's BNF (or EBNF).
  \item[Semantical analysis] - the next step, usually performed in unison with the previous step,
  in which the legal token sequences are given their semantic meaning.
\end{description}
Specifically, the proposal is that API design of follows the footsteps of
Accordingly, the designer of a fluent API has to follow these three conceptual
steps.
First is the identification of the \emph{vocabulary}, i.e.,
the set of method calls including type arguments that may take part in the
fluent API\@.
In this fluent API example
\begin{JAVA}
allowing (any(Object.class))
  ¢¢.method("get.*")
  ¢¢.withNoArguments();
\end{JAVA}
then, there are three method calls, and the vocabulary has three items in it.
\begin{itemize}
  \item~$ℓ₁ = \cc{any(Class<?>)}$
  \item~$ℓ₂ = \cc{allowing($ℓ₁$)}$
  \item~$ℓ₃ = \cc{method(String)}$
  \item~$ℓ₄ = \cc{withNoArguments()}$
\end{itemize}
