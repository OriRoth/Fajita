\title{Formal Language Recognition with the Java Type Checker}

\documentclass[a4paper,USenglish]{lipics}
\usepackage{\jobname}

\author{Joseph (Yossi) Gil \& Tomer Levy
%  Department of Computer Science⏎
%  Technion---Israel Institute of Technology⏎
%  \texttt{\small
%%    \href{mailto:stlevy@campus.technion.ac.il}{stlevy@campus.technion.ac.il}
%    \qquad
%    \href{mailto:yogi@campus.technion.ac.il}{yogi@campus.technion.ac.il}
%  }
}

\begin{document}

\section{The JLR Recognizer}
\input algorithm
\end{document}

\maketitle
\hfill
\parbox{40ex}{%
  \begin{flushright}
    \scriptsize\itshape ``\NonCitingUse{Java} generics are 100\% pure syntactic sugar,
    and do not support meta-programming''\footnotemark
  \end{flushright}
}
\newline

\footnotetext{Found on stackoverflow:
  \tiny
      \url{http://programmers.stackexchange.com/questions/95777/generic-programming-how-often-is-it-used-in-industry}
    }

\begin{abstract}
  \input{abstract}
\end{abstract}

\section{Introduction}
\input{aa}

\textbf{Outline.} \small
\Cref{Section:fluent} is a brief reminder of method chaining, 
  and fluent APIs, accompanied a discussion of how this work is related to type states.
It is followed by a similar reminder of context-free languages, pushdown automata, 
  and such in \cref{Section:pushdown}.
Based on the vocabulary established this far, 
  the main result is stated in~\cref{Section:result}.
  
Towards the proof in \cref{Section:proof}, \cref{Section:toolkit} 
  shows idioms and techniques for encoding computation with    
  the \Java type-checker.
\Cref{Section:jump} makes use of these for encoding 
  ``jump-stack'', a non-trivial data-structure,
  which is used, with suitable modifications, in the proof. 

In \cref{Section:applicability}, we discuss the challenges in
  translating the proof into a compiler-compiler for fluent APIs.
In particular, this section demonstrates our claim (that may be
  surprising to some) that the standard \Java compiler may spend
  an exponential time on compiling rather simple programs.
\cref{Section:zz} concludes with directions for further research.
\normalsize

\section{Method Chaining, Fluent APIs, and, Type States}
\label{Section:fluent}
\input{fluent}

\section{Context-Free Languages and Pushdown Automata: Reminder and Terminology}
\label{Section:pushdown}
\input{pushdown}

\section{Statement of the Main Result} 
\label{Section:result}
\input{result}

\section{Techniques of Type Encoding}
\label{Section:toolkit}
\input{toolkit}

\section{The Jump-Stack Data-Structure}
\label{Section:jump}
\input{jump}

\section{Proof of \Cref{Theorem:Gil-Levy}}
\label{Section:proof}
\input{proof}

\section{The Prefix Theorem}
\label{Section:prefix}
\input{prefix}

\section{Notes on Practical Applicability}
\label{Section:applicability}
\input{applicability}

\section{Conclusion and Future Work}
\label{Section:zz}
\input{zz}

% \textbf{Acknowledgment.}
% Inspiring correspondence with Gilad Bracha is gratefully acknowledged. 


\bibliographystyle{abbrv}\small
\bibliography{author-names,other-shorthands-abbreviated,%
 publishers-abbreviated,%
 conferences-abbreviated,%
 journals-abbreviated,journals-full,%
 yogi-book,yogi-practice,yogi-journal,yogi-tr,%
 GPCE,OOPSLA,PLDI,USENIX,ECOOP,%
 00,yogi-confs}	

\clearpage
\appendix
\section{The JLR Recognizer}
\input algorithm

\end{document}


Processing programming languages
\begin{description}
 \item[Lexical analysis] - the first step of the process in which the character strings generated by the
 programmer are aggregated to the abstract tokens defined by the language designer.
 \item[Syntactical analysis (parsing) ] - the second step, in which the processed strings of tokens
 conform to the rules of a formal grammar defined by the language's BNF (or EBNF).
 \item[Semantical analysis] - the next step, usually performed in unison with the previous step,
 in which the legal token sequences are given their semantic meaning.
\end{description}
Specifically, the proposal is that API design of follows the footsteps of
Accordingly, the designer of a fluent API has to follow these three conceptual
steps.
First is the identification of the \emph{vocabulary}, i.e.,
the set of method calls including type arguments that may take part in the
fluent API\@.
In this fluent API example
\begin{JAVA}
allowing (any(Object.class))
 ¢¢.method("get.*")
 ¢¢.withNoArguments();
\end{JAVA}
then, there are three method calls, and the vocabulary has three items in it.
\begin{itemize}
 \item~$ℓ₁ = \cc{any(Class<?>)}$
 \item~$ℓ₂ = \cc{allowing($ℓ₁$)}$
 \item~$ℓ₃ = \cc{method(String)}$
 \item~$ℓ₄ = \cc{withNoArguments()}$
\end{itemize}
