\title{%
  Formal Language Recognition with The Java Type Checker 
  \newline
  \color{red}{%
    \rmfamily\scshape Thou Mortal, Be Warned. \newline
    Thou Shallt Not Remove \newline
    This Commandment \newline
    While There Are Signs of Haste \newline
    in This Document!!!!\newline
  }
}

\documentclass[a4paper,USenglish]{lipics}
\usepackage{\jobname}

%\author{Tome Levy⏎
% Department of Computer Science⏎
% Technion---Israel Institute of Technology⏎
% \texttt{\small \href{mailto:stlevy@campus.technion.ac.il}{stlevy@campus.technion.ac.il}}}

\author{Anonymized for the submission}

\begin{document}

\maketitle
\begin{abstract}
  \input{abstract}
\end{abstract}

\section{Introduction}
\input{aa}

\textbf{Outline.}
The main result is presented in~\cref{Section:proof}.
Towards this,~\cref{Section:preliminaries}, the preliminaries, 
  recalls the central notions we rely on: DSL, fluent API,
  context free languages, pushdown automata, etc. 
At the end of this section, the accumulated vocabulary is used to state this
  result more formally.
This terminology is used again in \Cref{Section:related} to offer 
  a perspective on related work.
\Cref{Section:toolkit} then builds a small toolkit of idioms and techniques
  for programming generics in \Java.
This toolkit is used in the subsequent~\cref{Section:proof} for
  proving our main theoretical result.
On~\Cref{Section:compiler} we will discuss our main computational tool, 
  e.g., the \Java compiler, and its expressiveness.

\section{Reminders and Preliminaries}
\label{Section:preliminaries}
\input{preliminaries}

\section{Statement of the Main Result}
\label{Section:result}
\input{result}

\section{Related Work}
\label{Section:related}
\input{related}

\section{Techniques of Type Encoding}
\label{Section:toolkit}
\input{toolkit}

\section{The Jump-Stack Data-Structure}
\label{Section:jump}
\input{jump}

\section{Proof of \Cref{Theorem:Gil-Levy}}
\label{Section:proof}
\input{proof}

\section{\Java Compiler expressiveness}
\label{Section:compiler}
\input{compiler}

\section{Conclusion}
\label{Section:zz}
\input{zz}

\bibliographystyle{abbrv}\small
\bibliography{author-names,other-shorthands,journals-full,publishers-abbreviated,%
  conferences-abbreviated,%
  journals-abbreviated,%
  yogi-book,yogi-practice,GPCE,OOPSLA,00}
\end{document}

Processing programming languages
\begin{description}
  \item[Lexical analysis] - the first step of the process in which the character strings generated by the
  programmer are aggregated to the abstract tokens defined by the language designer.
  \item[Syntactical analysis (parsing) ] - the second step, in which the processed strings of tokens
  conform to the rules of a formal grammar defined by the language's BNF (or EBNF).
  \item[Semantical analysis] - the next step, usually performed in unison with the previous step,
  in which the legal token sequences are given their semantic meaning.
\end{description}
Specifically, the proposal is that API design of follows the footsteps of
Accordingly, the designer of a fluent API has to follow these three conceptual
steps.
First is the identification of the \emph{vocabulary}, i.e.,
the set of method calls including type arguments that may take part in the
fluent API\@.
In this fluent API example
\begin{JAVA}
allowing (any(Object.class))
  ¢¢.method("get.*")
  ¢¢.withNoArguments();
\end{JAVA}
then, there are three method calls, and the vocabulary has three items in it.
\begin{itemize}
  \item~$ℓ₁ = \cc{any(Class<?>)}$
  \item~$ℓ₂ = \cc{allowing($ℓ₁$)}$
  \item~$ℓ₃ = \cc{method(String)}$
  \item~$ℓ₄ = \cc{withNoArguments()}$
\end{itemize}
