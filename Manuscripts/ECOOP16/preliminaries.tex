\subsection{Method Chaining \emph{vs.} Fluent API: Reminder and Terminology}
The pattern ‟invoke function on variable \cc{sb}”, specifically with
  a function named \cc{append}, occurs
six times in the code in \cref{Figure:chaining}(a).

\begin{figure}[H]
  \caption{\label{Figure:chaining}%
    Recurring invocations of the pattern ‟invoke function on the same
      receiver”, before, and after method chaining.
  }%
    \begin{tabular}{@{}cc@{}}%
  \begin{lcode}[minipage,width=44ex,box align=center]{Java}
String time(int hours, int minutes, int seconds) {¢¢
  final StringBuilder sb = new StringBuilder();
  sb.append(hours);
  sb.append(':');
  sb.append(minutes);
  sb.append(':');
  sb.append(seconds);
  return sb.toString();
}\end{lcode}
\hfill
&
\hspace{1ex}
  \begin{lcode}[minipage,width=44ex,box align=center]{Java}
String time(int hours, int minutes, int seconds) {¢¢
    return new StringBuilder()
      ¢¢.append(hours).append(':')
      ¢¢.append(minutes).append(':')
      ¢¢.append(seconds)
      ¢¢.toString();
}\end{lcode}
⏎
\textbf{(a)} before & \textbf{(b)} after
\end{tabular}
\end{figure}

Some languages, e.g., \Smalltalk offer syntactic sugar, called \emph{method cascading}, for this pattern.
Method chaining is a ‟programmer made” syntactic sugar for this pattern:
  If a method~$f$ returns its receiver, i.e., \kk{this},
  then, instead of the series of two commandsr: \mbox{\cc{o.$f$(); o.$g$();}}, clients can write
  only one: \mbox{\cc{o.$f$().$g$();}}.
  \cref{Figure:chaining}(b) is the method chaining
  (also, shorter and arguably clearer) version of
  \cref{Figure:chaining}(a).
It is made possible thanks to the designer of class \cc{StringBuilder} who made sure that all overloaded variants of
  of \cc{append} return their receiver.

The distinction between fluent API and method chaining is the identity of the receiver:
In method chaining, all methods are invoked on the same object, whereas in fluent API
  the receiver of each method in the chain may be arbitrary.
Fluent API are more interesting for this reason.
Consider, for example, the following JAVA fragment (drawn from JMock~\cite{Freeman:Pryce:06})
\begin{JAVA}
allowing (any(Object.class))
  ¢¢.method("get.*")
  ¢¢.withNoArguments();
\end{JAVA}
Let the return type of function \cc{allowing} be denoted by~$τ₁$ and let the
  return type of function \cc{method} be denoted by~$τ₂$.
Then, the fact that~$τ₁≠τ₂$ means that the set of methods that can be placed after the dot
  in the partial call chain
\begin{JAVA}
allowing(any(Object.class)).
\end{JAVA}
is distinct from the set of methods that can be placed after the dot in the partial call chain
\begin{JAVA}
allowing(any(Object.class)).method("get.*").
\end{JAVA}
This distinction makes it possible to design expressive and rich fluent APIs, in which a
  sequence ‟chained” calls is not only readable, but also robust, in the sense that the
  sequence is type correct, only when it the sequence makes sense semantically.

\subsection{Context Free Languages and Pushdown Automata: Reminder and Terminology}
Each of the notions discussed here is probably common knowledge
 (see e.g.,~\cite{Hopcroft:book:2001,Linz:2001} for more precise definitions).
The purpose here is to set a unifying common vocabulary.

Let~$Σ$ be a finite alphabet of \emph{terminals} (often called input characters or tokens).
A \emph{language} over~$Σ$
  is a subset of~$Σ^*$ (the set of all strings, including the empty string,
  whose characters are drawn from~$Σ$).
Keep~$Σ$ implicit henceforth.

A \emph{\textbf Nondeterministic \textbf Pushdown \textbf Automaton} (NPDA) is a device for language recognition,
  made of a non-deterministic finite automaton
  and a stack of unbounded depth of stack symbols.
An NPDA begins execution with a single copy of the initial stack symbol on the stack.
In each step, the NPDA
  examines the next input token,
  the state of the automaton,
  and the top of the stack.
It then pops the top symbol in the stack, and nondeterministically chooses which actions of
  its transition function to perform:
  Consuming the next input token,
popping the current state from the stack,
  or, pushing a new state into it.
Non-determinism effectively means
  that any number of these can be done.

The language recognized by an NPDA is the set of strings that it accepts,
  either by reaching an accepting state or by encountering an empty stack.

A \emph{\textbf Context-\textbf Free \textbf Grammar}(CFG) is a formal description of a language.
A CFG~$G$ has three components:~$Ξ$ a set of \emph{variables} (also called nonterminals),
  a unique \emph{start variable}~$ξ∈Ξ$, and a finite set of (production) \emph{rules}.
A rule~$r∈G$ describes the derivation of a variable~$ξ∈Ξ$ into
  a string of \emph{symbols}, where symbols are either terminals or variables.
Accordingly, rule~$r∈G$ is written as~$r=ξ→β$, where~$β∈\left(Σ∪Ξ\right)^*$.
This description is often called BNF\@.
The \emph{language} of a CFG is the set of strings of terminals (and terminals only)
  that can be derived from the start symbol, following any sequence of applications of the rules.
CFG languages include regular languages, and are strictly contained in the set
  of ‟context-sensitive” languages.

The expressive power of NPDAs and BNFs is the same:
  For every language defined by a BNF, there exists an NPDA that recognizes it.
Conversely, there is a BNF definition for any language recognized by some NPDA.

NPDAs run in exponential deterministic time.
A more sane, but weaker, alternative is offered by LR($1$) parsers, 
  which are deterministic linear time and space.
An LR($1$) parser for a grammar~$G$ has three components:
\begin{description}
  \item[An automaton] whose states are~$Q=❴q₀,…,qₙ❵$
  \item[A stack] which may contain any member of~$Q$.
  \item[Specification of state transition] with the aid of two tables:
        \begin{description}
          \item[Goto table] which defines a partial function~$δ:Q⨉Ξ↛Q$ of transitions
          between the states of the automaton.
          \item[Action table] which
            defines a partial function\[η:Q⨉Σ↛ ❴ \textsf{Shift}(q) \,|\, q∈Q❵ ∪ ❴\textsf{Reduce}(r) \,| \, r∈G❵.\]
        \end{description}
\end{description}
The parser begins by pushing~$q₀$ (the initial state) into the stack,
and then repetitively executes the following:
Examine~$q∈Q$, the state at the top of the stack.
If~$q=qₙ$ (the accepting state) and the input string is wholly consumed, then the parser stops and the input is accepted.
Let~$σ∈Σ$ be the next input symbol.
If~$η(q,σ)=⊥$ (undefined), the parser stops in rejecting the input.
If~$η(q,σ) = \textsf{Shift}(q')$, the parser pushes state~$q'$ into the stack.
If however,~$a(q,σ) = \textsf{Reduce}(r)$,
where~$r=ξ→β$, the parser does not consume~$σ$ from the input string,
  and pops~$|β|$ stack symbols from the stack.
Let~$q'$ be the state at the top of the stack after these pops, then
  the parser pushes a state,~$q”$,
  defined by applying the transition function on~$ξ$, the left-hand side of the reduced production and~$q'$, i.e.,~$q”=δ(q',ξ)$.

The specifics of parser generation, including notions such the famous LR items lie beyond our interest here. 
We do mention however that the same LR($1$) parser can serve parsing table drawn using weaker LR algorithms, including SLR and
LALR($1$) (which \Self uses).

More general, LR($k$) parsers,~$k>1$, can be defined. These make their
  decisions based on the next~$k$ input symbols, rather than just the first of these.
General LR($k$) parsers are rarely used, since they offer essentially
  the same expressive power†{they recognize the same set of languages~\cite{Knuth:65}}, at a greater toll on resources.

To characterize the expressive power of LR($1$) parsers, we need the notion
of ‟\emph{\textbf Deterministic \textbf Pushdown \textbf Automaton}” (DPDA).
DPDA are similar to NPDA, except that they are forced
  to make deterministic choices.
<<<<<<< HEAD
The more formal and detailed definition is:
=======
We propose a slightly different definition from the classic:,
>>>>>>> f3f1f7dd796d9e7d4d6a0ec84fd85c4e0f3cf6db
\begin{Definition}[Deterministinc Pushdown Automaton]
  \label{Definition:DPDA}
  A \emph{deterministic pushdown automaton} (DPDA),~$M$, is a 6-tuple~$M =⟨Q,Γ, q₀,⊥, A,δ⟩$
  where~$Q$ is a finite set of
  \emph{the states of~$M$},~$Γ$ is a finite
  \emph{set of stack elements},~$q₀∈Q$ is the initial state,~$⊥∈Γ$
  is a \emph{special symbol designating the bottom of the stack}
<<<<<<< HEAD
  and~$A⊆Q$ is the \emph{set of accepting states} while~$δ$ is
  the \emph{partial functions of state transition}~$δ: (Γ∪❴ε❵)⨉Q⨉Σ ↛ Q⨉Γ^*.~$.

A DPDA begins its work in the initial state with a designated stack symbol residing on the stack.
At each step, the automaton examines:~$a$, the next input token, the current state~$q∈Q$, and the element~$γ∈Γ$ at the top of the stack and based on
  their values, decides how to proceed.

If~$γ=⊥$, the DPDA accepts the input and halts.
The same happens if~$q∈A$.
Suppose that~$δ(ε,q,γ) \ne \bot$. 
(In this case, the definition requires that~$δ(a',q,γ)=\bot$ for any~$a'∈Σ$
Let~$δ(ε,q,γ)=(q',ζ)$, then as before, the automaton pops~$γ$,
  pushes~$ζ$ into the stack, but also irrevocably consumes the token~$a$.

If~$δ(ε,q,γ)=δ(a,q,γ)=\bot$, then 
the automaton rejects the input and stops.
=======
  and~$A⊆Q$ is the \emph{set of accepting states} while~$δ$ and~$η$ are
  the \emph{partial functions of state transition}
  \[
    \begin{array}{crlc}
      δ: & Q⨉Σ⨉Γ &↛& Q⨉Γ^*⏎
      η: & Q⨉Γ &↛& Q⨉Γ^*,⏎
    \end{array}
  \]
  $η$ is the partial function for epsilon moves, and $δ$ is the remaining of the classic transition function.
>>>>>>> f3f1f7dd796d9e7d4d6a0ec84fd85c4e0f3cf6db
\end{Definition}

As mentioned above, NPDA languages are the same as CFG languages.
It is convenient to speak also of DCFG languages, the \emph{deterministic context-free grammar} languages,
  which are those context-free languages that are recognizable by a DPDA.

DCFG languages are a strict container of regular languages,
  and are strictly contained in CFG languages.
On the other hand, DPDA are easier to parse:
  Recognition of a DPDA language
  can be done in linear time and one pass.
  In contrast, CYK, one of the best algorithms for recognizing NPDA languages run in super-quadratic time~\cite{Younger:1967,Cocke:1969,Earley:1970}.

Where do LR($1$) languages stand with respect to DCFG context free languages?
LR($1$) languages are indeed equivalent to DCFG languages, but
the answer hinges on the distinction
  between LR($1$) languages and LR($1$) grammars.
To actually produce an LR($1$) parser for a given language,
  one needs to find an ‟\emph{LR($1$) grammar}” for the language.
Such a grammar is amenable to
  the automatic production of an LR($1$) parser.
However, the time for obtaining this LR($1$) grammar and 
  then processing it to produce an LR($1$) parser,
  as well as resulting parser's size,
  though polynomial, are prohibitive.


Where does \Self compare in terms of grammar expressive power with a popular LL($*$)~\cite{Parr:2011} parser generators such
  as ANTLR~\cite{Parr:1995}?
<<<<<<< HEAD
  As it turns out, languages of LL(*) do not contain the set of LALR(1) languages†{LL(*) language fail to handle left-recursive 
    grammars~\cite{Parr:2011}.},
neither are they contained in it. If fact LL(*) languages
  contain even some non-deterministic context free languages.
=======
As it turns out, LL($*$) grammars do not contain the set of LR($1$) grammars~†{As LL($*$) does not contain left-recursive grammars~\cite{Parr:2011}.}, 
  neither are they contained in it. In fact some context-sensitive 
  languages can be parsed by LL($*$) parsers.
>>>>>>> f3f1f7dd796d9e7d4d6a0ec84fd85c4e0f3cf6db
