\subsection{Method Chaining \emph{vs.} Fluent API: Reminder and Terminology}
The pattern ‟invoke function on variable \cc{sb}”, specifically with
  a function named \cc{append}, occurs
six times in the code in \cref{Figure:chaining}(a).

\begin{figure}[H]
  \caption{\label{Figure:chaining}%
    Recurring invocations of the pattern ‟invoke function on the same
      receiver”, before, and after method chaining.
  }%
    \begin{tabular}{@{}cc@{}}%
  \begin{lcode}[minipage,width=44ex,box align=center]{Java}
String time(int hours, int minutes, int seconds) {¢¢
  final StringBuilder sb = new StringBuilder();
  sb.append(hours);
  sb.append(':');
  sb.append(minutes);
  sb.append(':');
  sb.append(seconds);
  return sb.toString();
}\end{lcode}
\hfill
&
\hspace{1ex}
  \begin{lcode}[minipage,width=44ex,box align=center]{Java}
String time(int hours, int minutes, int seconds) {¢¢
    return new StringBuilder()
      ¢¢.append(hours).append(':')
      ¢¢.append(minutes).append(':')
      ¢¢.append(seconds)
      ¢¢.toString();
}\end{lcode}
⏎
\textbf{(a)} before & \textbf{(b)} after
\end{tabular}
\end{figure}

Some languages, e.g., \Smalltalk offer syntactic sugar, called \emph{method cascading}, for this pattern.
Method chaining is a ‟programmer made” syntactic sugar for this pattern:
  If a method~$f$ returns its receiver, i.e., \kk{this},
  then, instead of the series of two commands \mbox{\cc{o.$f$(); o.$g$();}}, clients can write
  only one command \mbox{\cc{o.$f$().$g$();}}.
  \cref{Figure:chaining}(b) is the method chaining
  (also, shorter and arguably clearer) version of
  \cref{Figure:chaining}(a).
It is made possible thanks to the designer of class \cc{StringBuilder} who made sure that all overloaded variants of
  of \cc{append} return their receiver.

The distinction between fluent API and method chaining is the identity of the receiver:
In method chaining, all methods are invoked on the same object, whereas in fluent API
  the receiver of each method in the chain may be arbitrary.
Fluent API are more interesting for this reason.
Consider, for example, the following JAVA fragment (drawn from JMock~\cite{Freeman:Pryce:06})
\begin{JAVA}
allowing (any(Object.class))
  ¢¢.method("get.*")
  ¢¢.withNoArguments();
\end{JAVA}
Let the return type of function \cc{allowing} be denoted by~$τ₁$ and let the
  return type of function \cc{method} be denoted by~$τ₂$.
Then, the fact that~$τ₁≠τ₂$ means that the set of methods that can be placed after the dot
  in the partial call chain
\begin{JAVA}
allowing(any(Object.class)).
\end{JAVA}
is distinct from the set of methods that can be placed after the dot in the partial call chain
\begin{JAVA}
allowing(any(Object.class)).method("get.*").
\end{JAVA}
This distinction makes it possible to design expressive and rich fluent APIs, in which a
  sequence ‟chained” calls is not only readable, but also robust, in the sense that the
  sequence is type correct, only when it the sequence makes sense semantically.

\subsection{Context Free Languages and Pushdown Automata: Reminder and Terminology}
Each of the notions discussed here is probably common knowledge
 (see e.g.,~\cite{Hopcroft:book:2001,Linz:2001} for more precise definitions).
The purpose here is to set a unifying common vocabulary.

Let~$Σ$ be a finite alphabet of \emph{terminals} (often called input symbols).
A \emph{language} over~$Σ$
  is a subset of~$Σ^*$ (the set of all strings, including the empty string,
  whose characters are drawn from~$Σ$).
Keep~$Σ$ implicit henceforth.

A \emph{\textbf Nondeterministic \textbf Pushdown \textbf Automaton} (NPDA) is a device for language recognition,
  made of a non-deterministic finite automaton
  and a stack of unbounded depth of stack symbols.
An NPDA begins execution when the finite automaton in the initial state, and the stack holds only the initial stack symbol.
In each step, the NPDA 
  examines the next input symbol,
  the state of the automaton,
  and the top of the stack.
It then pops the top symbol in the stack, and nondeterministically chooses what to perform (must conform with the transition function) : 
  it might consume the input symbol,
  it might push new stack symbols, 
  it might move to a new state,
  and every combination of the three.

The language recognized by an NPDA is the set of strings that it accepts,
  either by reaching an accepting state or by encountering an empty stack.

A \emph{\textbf Context-\textbf Free \textbf Grammar}(CFG) is a formal description of a language.
A CFG~$G$ has four components: an \emph{alphabet}~$\Sigma$,
a set of \emph{variables}~$Ξ$(also called nonterminals), a unique \emph{start variable}~$ξ∈Ξ$, and
  a finite set of (production) \emph{rules}.
A rule~$r∈G$ describes the derivation of a variable~$ξ∈Ξ$ into
  a string of \emph{symbols}, where symbols are either terminals or variables.
Accordingly, rule~$r∈G$ is written as~$r=ξ→β$, where~$β∈\left(Σ∪Ξ\right)^*$.
This description is often called BNF.
The \emph{language} of a CFG is the set of strings of terminals (and terminals only)
  that can be derived from the start symbol, following any sequence of applications of the rules.
CFG languages include regular languages, and are strictly contained in the set
  of ‟context-sensitive” languages.

The expressive power of NPDAs and BNFs is the same:
  For every language defined by a BNF, there is an NPDA that recognizes it.
Conversely, there is a BNF definition for any language recognized by some NPDA.

NPDAs run in exponential deterministic time.
A more sane, but weaker, alternative is offered by LR(1) parsers.
An LR(1) parser for a grammar~$G$ has three components:
\begin{description}
  \item[An automaton] whose states are~$Q=❴q₀,…,qₙ❵$
  \item[A stack] which may contain any member of~$Q$.
  \item[Specification of state transition] with the aid of two tables:
        \begin{description}
          \item[Goto table] which defines a partial function~$δ:Q⨉Ξ↛Q$ of transitions
          between the states of the automaton.
          \item[Action table] which
            defines a partial function\[η:Q⨉Σ↛ ❴ \textsf{Shift}(q) \,|\, q∈Q❵ ∪ ❴\textsf{Reduce}(r) \,| \, r∈G❵.\]
        \end{description}
\end{description}
The parser begins by pushing~$q₀$ (the initial state) into the stack,
and then repetitively executes the following:
Examine~$q∈Q$, the state at the top of the stack.
If~$q=qₙ$ (the accepting state), then the parser stops in accepting the input.
Let~$σ∈Σ$ be the next input symbol.
If~$η(q,σ)=⊥$, the parser stops in rejecting the input.
If~$η(q',σ) = \textsf{Shift}(q')$, the parser pushes state~$q'$ into the stack.
If however,~$a(q,σ) = \textsf{Reduce}(r)$,
where~$r=ξ→β$,
the parser pops~$|β|$ stack symbols from the stack.
Let~$q'$ be the state at the top of the stack after these pops.
The parser then pops~$q'$, the new state it reached,
  and pushes instead a state,~$q”$,
  defined by applying the transition function on~$ξ$, the variable just discovered and~$q'$ , i.e.,~$q”=δ(q',ξ)$.

More general, LL($k$) parsers,~$k>1$, can be defined. These make their
  decisions based on the next~$k$ input symbols, rather than just the first of these.
  General LL($k$) parsers are rarely used, since they offer essentially 
    the same expressive power as LL(1) parsers, at a greater toll on resources.

To characterize the expressive power of LR(1) parsers, we need the notion
of ‟\emph{\textbf Deterministic \textbf Pushdown \textbf Automaton}” (DPDA).
DPDA are similar to NPDA, except that they are forced
  to make deterministic choices.
More formally,
\begin{Definition}[Deterministinc Pushdown Automaton]
  \label{Definition:DPDA}
  A \emph{deterministic pushdown automaton} (DPDA),~$M$, is a 7-tuple
  \[
    M =⟨Q,Γ, q₀,⊥, A,δ,η⟩
  \]
  where~$Q$ is a finite set of
  \emph{the states of~$M$},~$Γ$ is a finite
  \emph{set of stack symbols},~$q₀∈Q$ is the initial state,~$⊥∈Γ$
  is a \emph{special symbol designating the bottom of the stack}
  and~$A⊆Q$ is the \emph{set of accepting states} while~$δ$ and~$η$ are
  the \emph{partial functions of state transition}
  \[
    \begin{array}{crlc}
      δ: & Q⨉Σ⨉Γ &↛& Q⨉Γ^*⏎
      η: & Q⨉Γ &↛& Q⨉Γ^*,⏎
    \end{array}
  \]
\end{Definition}

As mentioned above, NPDA languages are the same as CFG languages.
It is convenient to speak also of DCFG languages, the \emph{deterministic context-free grammar} languages,
  which are those context-free languages recognizable by DPDA.

DCFG languages are a strict container of regular languages, 
  and are strictly contained in CFG languages.
  On the other hand, DPDA are easier to parse.
Recognition of a DPDA language can be done in linear time and one pass.
  In contrast, one of the best algorithms for recognizing NPDA languages run in super-quadratic time~\cite{Younger:1967,Cocke:1969,Earley:1970}.

Where do LR(1) languages stand with respect to DCFG context free languages?
LR(1) languages are indeed equivalent to DCFG languages, but
the answer hinges on the distinction
  between LR(1) languages and LR(1) grammars.
To actually produce an LR(1) parser for a given language,
  one needs to find an‟\emph{LR(1) grammar}”, for the language.
Such a grammar is amenable to
  the automatic production of an LR(1) parser.
However, the time for obtaining this LR(1) grammar and the processing it to produce an LR(1) parser,
  as well as parser's size,
  though polynomial, are prohibitive.

Where does \Self compare in terms of expressive power with of popular LL(*)~\cite{Parr:2011} parser generators such %it was introduced there. you can see at Mendeley, i highlighted it.
  as ANTLR~\cite{Parr:1995}?
  As it turns out, languages of LL(*) do not contain the set of LALR(1) languages~\footnote{As it can't handle left-recursive grammars~\cite{Parr:2011}.}, 
neither are they contained in it. If fact LL(*) languages
  contain even some non-deterministic context free languages.