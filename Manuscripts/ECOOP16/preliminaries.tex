\subsection{Method Chaining \emph{vs.} Fluent API}
The pattern ‟invoke function on variable \cc{sb}” occurs
six times in the code in \cref{Figure:chaining}(a).

\begin{figure}[H]
  \caption{\label{Figure:chaining}%
    Recurring invocations of the pattern ‟invoke function on the same
      receiver”, before, and after method chaining.
  }
  \begin{tabular}{cc}
  \begin{lcode}[minipage,width=44ex,box align=center]{Java}
String time(int hours, int minutes, int seconds) {¢¢
  StringBuilder sb = new StringBuilder();
  sb.append(hours);
  sb.append(':');
  sb.append(minutes);
  sb.append(':');
  sb.append(seconds);
  return sb.toString();
}\end{lcode}
\hfill
&
  \begin{lcode}[minipage,width=44ex,box align=center]{Java}
String time(int hours, int minutes, int seconds) {¢¢
    return new StringBuilder()
      ¢¢.append(hours).append(':')
      ¢¢.append(minutes).append(':')
      ¢¢.append(seconds)
      ¢¢.toString();
}\end{lcode}
\\
\textbf{(a)} before & \textbf{(b)} after 
\end{tabular}
\end{figure}

Some languages, e.g., \Smalltalk offer syntactic sugar, called \emph{method cascading}, for this pattern.
Method chaining is a ‟programmer made” syntactic sugar for this pattern:
  If a method~$f$ returns its receiver, i.e., \kk{this},
  then, instead of the series of two commands \mbox{\cc{o.$f$(); o.$g$();}}, clients can write
  only one command \mbox{\cc{o.$f()$.$g$();}}.
  \cref{Figure:chaining}(b) is the method chaining
  (also, shorter and arguably clearer) version of
  \cref{Figure:non-chaining}(a).
It is made possible by the designer of class \cc{StreamBuilder} making sure that all overloaded variants of
  of \cc{append} return their receiver.


Now, definition found in the web of the term \emph{fluent API†{API =
    \textbf Application \textbf Program \textbf Interface
}} are a bit illusive.
Invariably, such definitions liken fluent API to method chaining;
  they also tend to emphasize that fluent API is ‟more” than method
  chaining, and ask the reader to extrapolate from one term to the other.

The distinction between the two terms is the identity of the receiver.
In method chaining, all methods are invoked on the same object, whereas in fluent API
  the receiver of each method in the chain may be arbitrary.
Perhaps surprisingly, this difference makes fluent API more expressive.
Consider, for example, the following JAVA fragment (drawn from JMock~\cite{Freeman:Pryce:06})
\begin{JAVA}
allowing (any(Object.class))
  ¢¢.method("get.*")
  ¢¢.withNoArguments();
\end{JAVA}
Let the return type of function \cc{allowing} be denoted by~$τ₁$ and let the
  return type of function \cc{method} be denoted by~$τ₂$.
Then, the fact that~$τ₁≠τ₂$ means that the set of methods that can be placed after the dot
  in the partial call chain
\begin{JAVA}
allowing(any(Object.class)).
\end{JAVA}
is distinct from the set of methods that can be placed after the dot in the partial call chain
\begin{JAVA}
allowing(any(Object.class)).method("get.*").
\end{JAVA}
This distinction makes it possible to design expressive and rich fluent APIs, in which a
  sequence ‟chained” calls is not only readable, but also robust, in the sense that the
  sequence is type correct, only when it the sequence makes sense semantically.

\subsection{Fluent API and the Type Safe Toilette Seat}

An object of type toilette seat is created in the \cc{down} state, but it can
then be \cc{raise}d to the \cc{up} state, and then be \cc{lower}ed to the
\cc{down} state.†{%
  This example is inspired by earlier work of
  Richard Harter on the topic~\cite{Harter:05}.
}
Such an object be used by two kinds of users, \cc{male}s and \cc{female}s, for two distinct purposes:
  \cc{urinate} and \cc{defecate}.
Now a good fluent API design is one by which the sequences of method calls in
  \cref{Figure:toilette:legal} are type correct.

\begin{figure}[htbp]
  \begin{JAVA}
new Seat().male().raise().urinate();
new Seat().female().urinate();\end{JAVA}
  \caption{Legal sequences of calls in the toilette seat example}
  \label{Figure:toilette:legal}
\end{figure}

Conversely, sequences of method calls made in \cref{Figure:toilette:illegal}
  should be illegal in the desired implementation of the fluent API.

\begin{figure}[htbp]
  \begin{JAVA}
new Seat().female().raise();
new Seat().male().raise().defecate();
new Seat().male().male();
new Seat().male().raise().urinate().female().urinate();\end{JAVA}
  \caption{Illegal sequences of calls in the toilette seat example}
  \label{Figure:toilette:illegal}
\end{figure}
It should be clear that the type checking engine of the compiler can
be employed to distinguish between legal and illegal sequences.
It should also be clear that fabricating the \kk{class}es, \kk{interface}s
and the \kk{extends} and \kk{implements} relationships between these, is
far from being trivial.

\subsection{What is fluent API?}

\subsection{Context Free Languages and Pushdown Automata: Reminder and Terminology}
Each of the notions discussed here is probably common knowledge
 (see e.g.,~\cite{Hopcroft:book:2001,must be others} for more precise definitions).
The purpose here is to set a unifying common vocabulary. 

Let~$Σ$ be a finite alphabet of \emph{terminals} (often called input symbols).
A \emph{language} over $Σ$
  is a subset of~$Σ^*$ (the set of all strings, including the empty string,
  whose characters are drawn from~$Σ$).
Keep~$Σ$ implicit henceforth.

A \emph{\textbf Nondeterministic \textbf Pushdown \textbf Automaton} (NPDA) is a device for language recognition, 
  made of a non-deterministic finite automaton 
  and a stack of unbounded depth of states.
An NPDA begins by pushing into the initial state into the empty stack.
In each step the NPDA examines the next input symbol and the state at the
  top of the stack.
It then non-deterministically between three options: to proceed to the next input symbol,  
  to pop a state from a stack, or to push a state into it. 

The language recognized by an NPDA is the set of strings the it accepts,
  either by reaching an empty stack or by encountering an empty state. 

A \emph{\textbf Context-\textbf Free \textbf Grammar}(CFG) is a formal description of a language.
A CFG~$G$ has three components:~$Ξ$,
a set of \emph{variables} (also called nonterminals), a unique \emph{start variable}~$ξ∈Ξ$, and
  a finite set of (production) \emph{rules}.
A rule~$r∈G$ describes the derivation of a variable~$ξ∈Ξ$ into
  a string of \emph{symbols}, where symbols are either terminals or variables.
Accordingly,~$r$ is written as~$r=ξ→β$, where~$β∈\left(Σ∪Ξ\right)^*$.
This description is often called BNF.
The \emph{language} of a CFG is the set of strings of terminals (and terminals only)
  that can be derived from the start symbol, following any sequence of applications of the rules.
CFG languages include regular languages, and are contained in the set
  of ‟context-sensitive” languages.

The expressive power of NPDAs and BNFs is the same: 
  For every language recognized by a BNF, there is an NPDA that recognizes it. 
Conversely, there is a BNF definition for any language recognized by some NPDA. 

NPDAs run in exponential deterministic time. 
A more sane, but weaker, alternative is offered by LR(1) parsers.  
An LR(1) parser for a grammar~$G$ has three components:
\begin{description}
  \item[An automaton] whose states are~$Q=❴q₀,…,qₙ❵$
  \item[A stack] which may contain any member of~$Q$.
  \item[Specification of state transition] with the aid of two tables:
        \begin{description}
          \item[Goto table] which defines a partial function~$δ:Q⨉Ξ↛Q$ of transitions
          between the states of the automaton.
          \item[Action table] which
            defines a partial function\[η:Q⨉Σ↛ ❴ \textsf{Shift}(q) \,|\, q∈Q❵ ∪ ❴\textsf{Reduce}(r) \,| \, r∈G❵.\]
        \end{description}
\end{description}
The parser begins by pushing~$q₀$ (the initial state) into the stack,
and then repetitively executes the following:
Examine~$q∈Q$, the state at the top of the stack.
If~$q=qₙ$ (the accepting state), then the parser stops in accepting the input.
Let~$σ∈Σ$ be the next input symbol.
If~$η(q,σ)=⊥$, the parser stops in rejecting the input.
If~$η(q',σ) = \textsf{Shift}(q')$, the parser pushes state~$q'$ into the stack.
If however,~$a(q,σ) = \textsf{Reduce}(r)$,
where~$r=ξ→β$,
the parser pops~$|β|$ stack symbols from the stack.
Let~$q'$ be the state at the top of the stack after these pops.
The parser then pops~$q'$, the new state it reached,
  and pushes instead a state,~$q”$, 
  defined by applying the transition function on~$ξ$, the variable just discovered and~$q'$ , i.e.,~$q”=δ(q',ξ)$.

More general, LL($k$)-parsers, $k>1$, can be defined. These make their
  decisions based on the next $k$ input symbols, rather than just the first of these.
These are rarely used, because they offer essentially the same expressive power as 
  LL(1) parsers, at a greater toll on resources. 

To characterize the expressive power of LR(1) parsers, we need the notion 
of ``\emph{\textbf Deterministic \textbf Pushdown \textbf Automaton}'' (DPDA). 
DPDA are similar to NPDA, except that they are forced 
  to make deterministic choices.
More formally, 
\begin{Definition}[Deterministinc Pushdown Automaton]
  \label{Definition:DPDA}
  A \emph{deterministic pushdown automaton} (DPDA),~$M$, is a 7-tuple
  \[
    M =⟨Q,Γ, q₀,⊥, A,δ,η⟩
  \]
  where~$Q$ is a finite set of
  \emph{the states of~$M$},~$Γ$ is a finite
  \emph{set of stack symbols},~$q₀∈Q$ is the initial state,~$⊥∈Γ$
  is a \emph{special symbol designating the bottom of the stack}
  and~$A⊆Q$ is the \emph{set of accepting states} while~$δ$ and~$η$ are
  the \emph{partial functions of state transition}
  \[
    \begin{array}{crlc}
      δ: & Q⨉Σ⨉Γ &↛& Q⨉Γ^*⏎
      η: & Q⨉Γ &↛& Q⨉Γ^*,⏎
    \end{array}
  \]
\end{Definition}

As mentioned above, NPDA languages are the same as CFG languages.
It is convenient to speak of also of DCFG languages, the \emph{deterministic context-free grammar} languages, 
  which are those context-free languages recognizable by DPDA.
  
DCFG languages are strictly contain regular languages, and are strictly contained
  in CFG languages.
DPDA are however easier to parse. Recognition of a DPDA language 
  can be done in linear time and one pass.
  In contrast, the best algorithms for recognizing NPDA languages run in super-quadratic time~\cite{CYK,I forget the nnames}. 

Where do LR(1) languages stand with respect to DCFG context free languages? 
LR(1) languages are indeed equivalent to DCFG languages, but 
the answer hinges on the distinction 
  between LR(1) languages and LR(1) grammars. 
To actually produce an LR(1) parser for a given language, 
  one needs to find an``\emph{LR(1) grammar}'', for the language.
Such a grammar is amenable to 
  the automatic production of an LR(1) parser.
However the time for obtaining this LR(1) grammar and the processing it, 
  though polynomial, may be prohibitive. 


\begin{wrapfigure}r{0.5\linewidth}
  \caption{ \label{Figure:expressiveness}
  Hierarchy of CFGs and pushdown automata}
  \input ../Figures/expressiveness-diagram.tikz
\end{wrapfigure}
In order to see the full picture of the expressiveness of the discussed classes, we will only
  only mention the LL($*$) class and it's expressiveness. The LL($*$)~\cite{Parr:2011} class does not contain,
  and neither contained by any of the LR($k$) grammar classes, in addition, LL($*$) can also
  parse some context-sensitive grammars.
\Cref{Figure:expressiveness} is a visualization of the discussed grammar's hierarchy.


